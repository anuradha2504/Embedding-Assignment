Text Embedding Techniques - NLP Assignment
This project explores various text embedding techniques using a sample dataset of product reviews. The goal is to understand how different embedding models represent textual data and to compare their effectiveness for downstream tasks.

ğŸ“ Dataset
The dataset used contains customer product reviews in a CSV format with the following fields:

review: Raw user review text.

cleaned_review: Preprocessed version of the review (lowercased, tokenized, lemmatized, etc.)

ğŸ”§ Preprocessing
Before applying embedding techniques, the text undergoes:

Lowercasing

Removal of punctuation and numbers

Tokenization

Stopword removal

Lemmatization

Removal of extra spaces

ğŸ§° Libraries Used
pandas, numpy â€” Data handling

nltk, re â€” Text preprocessing

sklearn â€” BoW, TF-IDF, PCA

gensim â€” Word2Vec, FastText

matplotlib, seaborn â€” Visualization

ğŸ“š Embedding Techniques Implemented
Technique	Description
Bag of Words	Converts text to word count matrix using CountVectorizer.
TF-IDF	Converts text to weighted frequency matrix using TfidfVectorizer.
Word2Vec	Trains embedding using Skip-gram or CBOW on tokenized reviews.
GloVe	Loads pre-trained GloVe embeddings and maps them to review vectors.
FastText	Trains embeddings using subword information (n-grams) with Gensim.

ğŸ§ª Analysis Performed
Cosine similarity checks between words

OOV (Out-of-Vocabulary) behavior comparison

PCA-based visualization of embeddings

Word similarity search across models

Qualitative comparison of techniques (context-awareness, handling misspellings, etc.)

ğŸ“Š Visualization Samples
Word embeddings visualized using PCA

Similar words for terms like cheap, love, bad, etc.

Review-level average embeddings

ğŸš€ How to Run
Open in Google Colab or Jupyter Notebook

Run the preprocessing cells to clean the dataset

Execute each embedding section one-by-one:

Bag of Words

TF-IDF

Word2Vec

GloVe

FastText

Review visualizations and comparisons

(Optional) Extend by applying embeddings to a classifier (e.g., sentiment analysis)

ğŸ“Œ Learning Outcomes
Understand the principles behind embedding models

Learn preprocessing steps required for text embeddings

Compare context-aware and context-unaware models

Visualize word relationships in vector space

ğŸ“ References
Gensim Documentation

Stanford GloVe

scikit-learn feature extraction

Author:
Anuradha Kumari
